{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "one_time_run = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import os\n",
    "import gc\n",
    "import codecs\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "%matplotlib inline\n",
    "pal = sns.color_palette()\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#Paths\n",
    "QUORA_DATA_DIR = \"/opt/datasets/quora/\"\n",
    "TRAIN_CSV = QUORA_DATA_DIR+'train.csv'\n",
    "TEST_CSV = QUORA_DATA_DIR+'test.csv'\n",
    "\n",
    "GLOVE_DATA_DIR=\"/opt/datasets/glove/\"\n",
    "glove_840B_300d = GLOVE_DATA_DIR+'glove.840B.300d.txt'\n",
    "GLOVE_DATA_FILE= glove_840B_300d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "EMBEDDING_DIM = 300\n",
    "MAX_SEQUENCE_LENGTH = 45\n",
    "MAX_NB_WORDS = 200000\n",
    "EMBEDDING_DIM = 300\n",
    "VALIDATION_SPLIT = 0.01"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['What is the step by step guide to invest in share market in india?', 'What is the story of Kohinoor (Koh-i-Noor) Diamond?', 'How can I increase the speed of my internet connection while using a VPN?', 'Why am I mentally very lonely? How can I solve it?', 'Which one dissolve in water quikly sugar, salt, methane and carbon di oxide?', 'Astrology: I am a Capricorn Sun Cap moon and cap rising...what does that say about me?', 'Should I buy tiago?', 'How can I be a good geologist?', 'When do you use シ instead of し?', 'Motorola (company): Can I hack my Charter Motorolla DCX3400?']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>qid1</th>\n",
       "      <th>qid2</th>\n",
       "      <th>question1</th>\n",
       "      <th>question2</th>\n",
       "      <th>is_duplicate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>What is the step by step guide to invest in sh...</td>\n",
       "      <td>What is the step by step guide to invest in sh...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>What is the story of Kohinoor (Koh-i-Noor) Dia...</td>\n",
       "      <td>What would happen if the Indian government sto...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>How can I increase the speed of my internet co...</td>\n",
       "      <td>How can Internet speed be increased by hacking...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>8</td>\n",
       "      <td>Why am I mentally very lonely? How can I solve...</td>\n",
       "      <td>Find the remainder when [math]23^{24}[/math] i...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>9</td>\n",
       "      <td>10</td>\n",
       "      <td>Which one dissolve in water quikly sugar, salt...</td>\n",
       "      <td>Which fish would survive in salt water?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  qid1  qid2                                          question1  \\\n",
       "0   0     1     2  What is the step by step guide to invest in sh...   \n",
       "1   1     3     4  What is the story of Kohinoor (Koh-i-Noor) Dia...   \n",
       "2   2     5     6  How can I increase the speed of my internet co...   \n",
       "3   3     7     8  Why am I mentally very lonely? How can I solve...   \n",
       "4   4     9    10  Which one dissolve in water quikly sugar, salt...   \n",
       "\n",
       "                                           question2  is_duplicate  \n",
       "0  What is the step by step guide to invest in sh...             0  \n",
       "1  What would happen if the Indian government sto...             0  \n",
       "2  How can Internet speed be increased by hacking...             0  \n",
       "3  Find the remainder when [math]23^{24}[/math] i...             0  \n",
       "4            Which fish would survive in salt water?             0  "
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train = pd.read_csv(TRAIN_CSV)\n",
    "df_test = pd.read_csv(TEST_CSV)\n",
    "\n",
    "#Train Data\n",
    "train_feature_1_string = pd.Series(df_train['question1'].tolist()).astype(str)\n",
    "train_feature_2_string = pd.Series(df_train['question2'].tolist()).astype(str)\n",
    "target = pd.Series(df_train['is_duplicate'].tolist())\n",
    "all_train_qs = train_feature_1_string + train_feature_2_string\n",
    "\n",
    "#Test Data\n",
    "test_feature_1_string = pd.Series(df_test['question1'].tolist()).astype(str)\n",
    "test_feature_2_string = pd.Series(df_test['question2'].tolist()).astype(str)\n",
    "all_test_qs = test_feature_1_string + test_feature_2_string\n",
    "\n",
    "all_qs = all_train_qs + all_test_qs\n",
    "\n",
    "print(train_qs.tolist()[:10])\n",
    "\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Text Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# plt.figure(figsize=(12, 5))\n",
    "# plt.hist(dt_all_qids.value_counts(), bins=50)\n",
    "# plt.yscale('log', nonposy='clip')\n",
    "# plt.title('Log-Histogram of question appearance counts')\n",
    "# plt.xlabel('Number of occurences of question')\n",
    "# plt.ylabel('Number of questions')\n",
    "# print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of question pairs for training: 404290\n",
      "Duplicate pairs: 36.92%\n",
      "Total number of questions in the training data: 537362\n",
      "Number of questions that appear multiple times: 111780\n",
      "Total number of questions in Quora dataset: 2345796\n"
     ]
    }
   ],
   "source": [
    "total_ques_pairs = len(train)\n",
    "print('Total number of question pairs for training: {}'.format(total_ques_pairs))\n",
    "\n",
    "duplicate_ques_pairs = round(df_train['is_duplicate'].mean()*100, 2)\n",
    "print('Duplicate pairs: {}%'.format(duplicate_ques_pairs))\n",
    "\n",
    "unique_qids = len(np.unique(train_qs))\n",
    "print('Total number of questions in the training data: {}'.format(unique_qids))\n",
    "\n",
    "print('Number of questions that appear multiple times: {}'.format(np.sum(all_qids.value_counts() > 1)))\n",
    "\n",
    "print(\"Total number of questions in Quora dataset: {}\".format(len(all_qs)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# dist_train = train_qs.apply(len)\n",
    "# dist_test = test_qs.apply(len)\n",
    "\n",
    "# plt.figure(figsize=(15, 10))\n",
    "# plt.hist(dist_train, bins=200, range=[0, 200], color=pal[2], normed=True, label='train')\n",
    "# plt.hist(dist_test, bins=200, range=[0, 200], color=pal[1], normed=True, alpha=0.5, label='test')\n",
    "# plt.title('Normalised histogram of character count in questions', fontsize=15)\n",
    "# plt.legend()\n",
    "# plt.xlabel('Number of characters', fontsize=15)\n",
    "# plt.ylabel('Probability', fontsize=15)\n",
    "\n",
    "# print('mean-train {:.2f} std-train {:.2f} mean-test {:.2f} std-test {:.2f} max-train {:.2f} max-test {:.2f} max-train{:.2f}'.format(dist_train.mean(), \n",
    "#                           dist_train.std(), dist_test.mean(), dist_test.std(), dist_train.max(), dist_test.max(), dist_train.max()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# from wordcloud import WordCloud\n",
    "# cloud = WordCloud(width=1440, height=1080).generate(\" \".join(train_qs.astype(str)))\n",
    "# plt.figure(figsize=(20, 15))\n",
    "# plt.imshow(cloud)\n",
    "# plt.axis('off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indexing word vectors.\n",
      "Found 2196017 word vectors.\n",
      "CPU times: user 4min 22s, sys: 2.87 s, total: 4min 24s\n",
      "Wall time: 4min 24s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "if(one_time_run):\n",
    "    print('Indexing word vectors.')\n",
    "    embeddings_index = {}\n",
    "    f = codecs.open(GLOVE_DATA_FILE, encoding='utf-8')\n",
    "    for line in f:\n",
    "        values = line.split(' ')\n",
    "        word = values[0]\n",
    "        coefs = np.asarray(values[1:], dtype='float32')\n",
    "        embeddings_index[word] = coefs\n",
    "    f.close()\n",
    "    print('Found %s word vectors.' % len(embeddings_index))\n",
    "else:\n",
    "    print('Skipped to save some time!')\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "one_time_run = False  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# embeddings_index['best']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# %%time\n",
    "# all_questions_text = train_feature_1_text + train_feature_2_text + test_feature_1_text + test_feature_2_text\n",
    "# all_questions_text = list(filter(lambda q: type(q) == str, all_questions_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# ques_lengths = list(map(lambda q: len(str(q).split(' ')) if(type(q) is str) else 0, all_questions_text))\n",
    "# #Beware the questions has nan and numbers\n",
    "\n",
    "# print(max(ques_lengths))   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def getTokenizeModel(list_of_all_string, max_number_words):\n",
    "    tokenizer = Tokenizer(nb_words = max_number_words)\n",
    "    tokenizer.fit_on_texts(dt_all_questions_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# %time\n",
    "# tokenizer = Tokenizer(nb_words=dt_MAX_NB_WORDS)\n",
    "# tokenizer.fit_on_texts(all_questions_text)\n",
    "# train_sequences_1 = dt_tokenizer.texts_to_sequences(train_feature_1_text)\n",
    "# train_sequences_2 = dt_tokenizer.texts_to_sequences(train_feature_2_text)\n",
    "# word_index = tokenizer.word_index\n",
    "# print('Found %s unique tokens.' % len(word_index))\n",
    "\n",
    "# dt_test_sequences_1 = tokenizer.texts_to_sequences(test_feature_1_text)\n",
    "# dt_test_sequences_2 = tokenizer.texts_to_sequences(test_feature_2_text)\n",
    "\n",
    "# data_1 = pad_sequences(train_sequences_1, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "# data_2 = pad_sequences(train_sequences_2, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "# labels = np.array(labels)\n",
    "# print('Shape of data tensor:', data_1.shape)\n",
    "# print('Shape of label tensor:', labels.shape)\n",
    "\n",
    "# test_data_1 = pad_sequences(test_sequences_1, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "# test_data_2 = pad_sequences(test_sequences_2, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "# test_labels = np.array(test_labels)\n",
    "# del test_sequences_1\n",
    "# del test_sequences_2\n",
    "# del train_sequences_1\n",
    "# del train_sequences_2\n",
    "# import gc\n",
    "# gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "l = [\"just say a word\",\"of some length\"]\n",
    "ll = list(map(lambda l: l.split(\" \"), l))\n",
    "seq_length = 5\n",
    "ll\n",
    "e = {\"just\": [0,0,0,0,1],\n",
    "     \"say\" : [0,0,0,1,1],\n",
    "     \"a\" : [0,1,1,1,0],\n",
    "     \"word\" : [1,0,0,1,0],\n",
    "     \"of\" : [0,1,0,1,0],\n",
    "     \"some\" : [0,1,0,1,0],\n",
    "     \"length\" : [0,0,1,1,0],\n",
    "     '<PAD>': [0,0,0,0,0]\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['just', 'say'], ['of', 'some']]\n",
      "[['just', 'say'], ['of', 'some']]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "bag_of_list_of_words_embed_transformer() missing 1 required positional argument: 'bag_list_of_words'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-105-206c447aaf6e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mlist_of_words_padding\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mll\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mlist_of_words_padding\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mll\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 27\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mbag_of_list_of_words_embed_transformer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlayer1_padding\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mll\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-105-206c447aaf6e>\u001b[0m in \u001b[0;36m<lambda>\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mlist_of_words_padding\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mll\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mlist_of_words_padding\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mll\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 27\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mbag_of_list_of_words_embed_transformer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlayer1_padding\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mll\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m: bag_of_list_of_words_embed_transformer() missing 1 required positional argument: 'bag_list_of_words'"
     ]
    }
   ],
   "source": [
    "def list_of_words_padding(list_of_words, seq_length=5, append=True):\n",
    "    \"\"\"Pads/slices given bag of words for specified length.\"\"\"\n",
    "    if len(list_of_words) == seq_length:\n",
    "        return list_of_words\n",
    "    if len(list_of_words) > seq_length:\n",
    "        return list_of_words[:seq_length]\n",
    "    \n",
    "    tmp = ['<PAD>' for i in range(seq_length - len(list_of_words))]\n",
    "    if append:\n",
    "        return list_of_words + tmp\n",
    "    return  tmp + list_of_words\n",
    "\n",
    "def list_of_words_embed_transformer(embed_dict, list_of_words):\n",
    "    \"\"\"Transform list of words to list of embed_dict values.\"\"\"\n",
    "    return map(lambda x:embed_dict(x), list_of_words)\n",
    "\n",
    "def bag_of_list_of_words_embed_transformer(embed_dict, bag_list_of_words):\n",
    "    \"\"\"Transform bag of list of words to bag of list of embed_dict values.\"\"\"\n",
    "    return map(list_of_words_embed_transformer, bag_list_of_words)\n",
    "\n",
    "def layer1_padding(list_of_words):\n",
    "    return list_of_words_padding(list_of_words=list_of_words, seq_length=7, append=True)\n",
    "\n",
    "\n",
    "print(list(map(lambda x: list_of_words_padding(x, 2, True), ll)))\n",
    "print(list(map(lambda x: list_of_words_padding(x, 2, False), ll)))\n",
    "print(list(map(lambda x: layer1_padding(x), ll)))\n",
    "\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['just', 'say', 'a', 'word', '', '', ''],\n",
       " ['of', 'some', 'length', '', '', '', '']]"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_feature(embed_dict, list_of_strings, max_seq_length, embed_dim):\n",
    "    features = np.zeros((len(list_of_strings), max_seq_length, embed_dim), dtype=float)\n",
    "    list_of_bag_of_words = map(lambda l: list_of_words_padding(list_of_words=l.split(),\n",
    "                                                                seq_length=max_seq_length,\n",
    "                                                                append=True),\n",
    "                                list_of_strings)\n",
    "    return list_of_bag_of_words\n",
    "\n",
    "    \n",
    "list(get_feature(e, l, 7, 5))    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Build the Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "lstm_size = 256\n",
    "lstm_layers = 1\n",
    "batch_size = 250\n",
    "learning_rate = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Create the graph object\n",
    "graph = tf.Graph()\n",
    "# Add nodes to the graph\n",
    "with graph.as_default():\n",
    "    inputs_ = tf.placeholder(tf.float32, [None, None, None], name='inputs') #[Number of ques, Seq/Ques Length, Embed Dims]\n",
    "    labels_ = tf.placeholder(tf.float32, [None, None], name='labels')\n",
    "    keep_prob = tf.placeholder(tf.float32, name='keep_prob')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "http://suriyadeepan.github.io/2016-06-28-easy-seq2seq/\n",
    "\n",
    "https://www.kaggle.com/c/word2vec-nlp-tutorial/details/part-1-for-beginners-bag-of-words\n",
    "\n",
    "https://github.com/yuhaozhang/sentence-convnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8.333333333333334"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "500 / 60\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
